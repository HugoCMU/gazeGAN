{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Style Transfer\n",
    "\n",
    "This notebook is used to train a style transfer network to convert simulated gaze data to real-looking gaze data using PyTorch.\n",
    "\n",
    "Sources:\n",
    "- [1] http://pytorch.org/tutorials/advanced/neural_style_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import copy\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "from PIL import Image\n",
    "from torchvision import datasets, models, transforms\n",
    "\n",
    "\n",
    "# Plotting options for notebook\n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "plt.ion() # Plotting in interactive mode\n",
    "# fig_size = [12, 6]\n",
    "# plt.rcParams[\"figure.figsize\"] = fig_size\n",
    "\n",
    "# Make sure we can use GPU\n",
    "use_gpu = torch.cuda.is_available()\n",
    "print('Gpu is enabled: %s' % use_gpu)\n",
    "dtype = torch.cuda.FloatTensor if use_gpu else torch.FloatTensor\n",
    "\n",
    "# Add local directories to path, get dataset utils\n",
    "ROOT_DIR = Path.cwd()\n",
    "sys.path.append(str(ROOT_DIR))\n",
    "from dataset_utils import load_single_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load synthetic and real image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local dataset directory\n",
    "data_dir = ROOT_DIR / '..' / 'local' / 'data'\n",
    "\n",
    "# Real and synthetic datasets\n",
    "synthetic_dataset = data_dir / '100118_fixedhead' / 'train'\n",
    "real_dataset = data_dir / '150118_gaze' / 'train'\n",
    "\n",
    "# grab random images from each dataset\n",
    "synthetic_image_path = str(list(synthetic_dataset.glob('*.png'))[0])\n",
    "real_image_path = str(list(real_dataset.glob('*.png'))[0])\n",
    "\n",
    "# Load images as variables (convert to gpu tensors using dtype variable)\n",
    "synth_image = load_single_image(synthetic_image_path, (96, 128)).type(dtype)\n",
    "real_image = load_single_image(real_image_path, (96, 128)).type(dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot images\n",
    "def plot_images(image_paths):\n",
    "    fig = plt.figure()\n",
    "    for i, image_path in enumerate(image_paths, 1):\n",
    "        # Use sublots to plot all of them\n",
    "        ax = plt.subplot(1, len(image_paths), i)\n",
    "        plt.tight_layout()\n",
    "        image = Image.open(image_path)\n",
    "        plt.imshow(image)\n",
    "        ax.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "plot_images([synthetic_image_path, real_image_path])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Custom Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContentLoss(nn.Module):\n",
    "    \"\"\"Content loss keeps images from changing their content. Inherit from PyTorch's nn module\"\"\"\n",
    "    \n",
    "    def __init__(self, target, weight):\n",
    "        super(ContentLoss, self).__init__()\n",
    "        # Detach target content from the tree used\n",
    "        # to dynamically compute the gradient: this is a stated value,\n",
    "        # not a variable. Otherwise the forward method of the criterion\n",
    "        # will throw an error.\n",
    "        self.target = target.detach()* weight\n",
    "        self.weight = weight\n",
    "        self.criterion = nn.MSELoss()\n",
    "        \n",
    "    def forward(self, input):\n",
    "        self.loss = self.criterion(input * self.weight, self.target)\n",
    "        self.output = input\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, retain_graph=True):\n",
    "        self.loss.backward(retain_graph=retain_graph)\n",
    "        return self.loss\n",
    "    \n",
    "class GramMatrix(nn.Module):\n",
    "    \"\"\"GramMatrix is used for Style Loss. Inherits from PyTorch's nn module\"\"\"\n",
    "    \n",
    "    def forward(self, input):\n",
    "        # batch size, feature maps, dimmensions of feature map\n",
    "        a, b, c, d = input.size()\n",
    "        # Reshape to vector\n",
    "        features = input.view(a * b, c * d)\n",
    "        G = torch.mm(features, features.t()) # Gram product\n",
    "        return G.div(a*b*c*d) # normalize and return\n",
    "    \n",
    "class StyleLoss(nn.Module):\n",
    "    \"\"\"Style loss makes the image styles more similar. Inherits from PyTorch's nn module\"\"\"\n",
    "    def __init__(self, target, weight):\n",
    "        super(StyleLoss, self).__init__()\n",
    "        self.target = target.detach() * weight\n",
    "        self.weight = weight\n",
    "        self.gram = GramMatrix()\n",
    "        self.criterion = nn.MSELoss()\n",
    "        \n",
    "    def forward(self, input):\n",
    "        self.output = input.clone()\n",
    "        self.G = self.gram(input)\n",
    "        self.G.mul_(self.weight)\n",
    "        self.loss = self.criterion(self.G, self.target)\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, retain_graph=True):\n",
    "        self.loss.backward(retain_graph=retain_graph)\n",
    "        return self.loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load feature extractor component of VGG19\n",
    "feature_extractor = models.vgg19(pretrained=True).features\n",
    "if use_gpu:\n",
    "    feature_extractor = feature_extractor.cuda()\n",
    "    \n",
    "# Style and content losses are calculated on specific layers\n",
    "content_layers_default = ['conv_1']\n",
    "style_layers_default = ['conv_1', 'conv_2', 'conv_3', 'conv_4', 'conv_5']\n",
    "\n",
    "def get_style_model_and_losses(cnn, style_img, content_img,\n",
    "                               style_weight=1000, content_weight=1,\n",
    "                               content_layers=content_layers_default,\n",
    "                               style_layers=style_layers_default):\n",
    "    cnn = copy.deepcopy(cnn)\n",
    "\n",
    "    # just in order to have an iterable access to or list of content/syle\n",
    "    # losses\n",
    "    content_losses = []\n",
    "    style_losses = []\n",
    "\n",
    "    model = nn.Sequential()  # the new Sequential module network\n",
    "    gram = GramMatrix()  # we need a gram module in order to compute style targets\n",
    "\n",
    "    # move these modules to the GPU if possible:\n",
    "    if use_gpu:\n",
    "        model = model.cuda()\n",
    "        gram = gram.cuda()\n",
    "\n",
    "    i = 1\n",
    "    for layer in list(cnn):\n",
    "        if isinstance(layer, nn.Conv2d):\n",
    "            name = \"conv_\" + str(i)\n",
    "            model.add_module(name, layer)\n",
    "\n",
    "            if name in content_layers:\n",
    "                # add content loss:\n",
    "                target = model(content_img).clone()\n",
    "                content_loss = ContentLoss(target, content_weight)\n",
    "                model.add_module(\"content_loss_\" + str(i), content_loss)\n",
    "                content_losses.append(content_loss)\n",
    "\n",
    "            if name in style_layers:\n",
    "                # add style loss:\n",
    "                target_feature = model(style_img).clone()\n",
    "                target_feature_gram = gram(target_feature)\n",
    "                style_loss = StyleLoss(target_feature_gram, style_weight)\n",
    "                model.add_module(\"style_loss_\" + str(i), style_loss)\n",
    "                style_losses.append(style_loss)\n",
    "\n",
    "        if isinstance(layer, nn.ReLU):\n",
    "            name = \"relu_\" + str(i)\n",
    "            model.add_module(name, layer)\n",
    "\n",
    "            if name in content_layers:\n",
    "                # add content loss:\n",
    "                target = model(content_img).clone()\n",
    "                content_loss = ContentLoss(target, content_weight)\n",
    "                model.add_module(\"content_loss_\" + str(i), content_loss)\n",
    "                content_losses.append(content_loss)\n",
    "\n",
    "            if name in style_layers:\n",
    "                # add style loss:\n",
    "                target_feature = model(style_img).clone()\n",
    "                target_feature_gram = gram(target_feature)\n",
    "                style_loss = StyleLoss(target_feature_gram, style_weight)\n",
    "                model.add_module(\"style_loss_\" + str(i), style_loss)\n",
    "                style_losses.append(style_loss)\n",
    "\n",
    "            i += 1\n",
    "\n",
    "        if isinstance(layer, nn.MaxPool2d):\n",
    "            name = \"pool_\" + str(i)\n",
    "            model.add_module(name, layer)  # ***\n",
    "\n",
    "    return model, style_losses, content_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_style_transfer(cnn, content_img, style_img, input_img, num_steps=800, style_weight=1000, content_weight=1):\n",
    "    \"\"\"Run the style transfer.\"\"\"\n",
    "    print('Building the style transfer model..')\n",
    "    model, style_losses, content_losses = get_style_model_and_losses(cnn,\n",
    "        style_img, content_img, style_weight, content_weight)\n",
    "    \n",
    "     # Show that input is a parameter that requires a gradient\n",
    "    input_param = nn.Parameter(input_img.data)\n",
    "    optimizer = optim.LBFGS([input_param])\n",
    "\n",
    "    print('Optimizing..')\n",
    "    run = [0]\n",
    "    while run[0] <= num_steps:\n",
    "\n",
    "        def closure():\n",
    "            # correct the values of updated input image\n",
    "            input_param.data.clamp_(0, 1)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            model(input_param)\n",
    "            style_score = 0\n",
    "            content_score = 0\n",
    "\n",
    "            for sl in style_losses:\n",
    "                style_score += sl.backward()\n",
    "            for cl in content_losses:\n",
    "                content_score += cl.backward()\n",
    "\n",
    "            run[0] += 1\n",
    "            if run[0] % 50 == 0:\n",
    "                print(\"run {}:\".format(run))\n",
    "                print('Style Loss : {:4f} Content Loss: {:4f}'.format(\n",
    "                    style_score.data[0], content_score.data[0]))\n",
    "                print()\n",
    "\n",
    "            return style_score + content_score\n",
    "\n",
    "        optimizer.step(closure)\n",
    "\n",
    "    # a last correction...\n",
    "    input_param.data.clamp_(0, 1)\n",
    "\n",
    "    return input_param.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the real image and run style transfer on it\n",
    "input_img = synth_image.clone()\n",
    "output = run_style_transfer(feature_extractor, synth_image, real_image, input_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnNormalize(object):\n",
    "    def __init__(self, mean, std):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __call__(self, tensor):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            tensor (Tensor): Tensor image of size (C, H, W) to be normalized.\n",
    "        Returns:\n",
    "            Tensor: Normalized image.\n",
    "        \"\"\"\n",
    "        for t, m, s in zip(tensor, self.mean, self.std):\n",
    "            t.mul_(s).add_(m)\n",
    "            # The normalize code -> t.sub_(m).div_(s)\n",
    "        return tensor\n",
    "\n",
    "# Convert output tensor back to image\n",
    "unorm = UnNormalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
    "to_PIL = transforms.ToPILImage(mode='RGB')\n",
    "image = unorm(output.cpu())\n",
    "image = to_PIL(image[0])\n",
    "            \n",
    "# Show final image\n",
    "plt.figure()\n",
    "plt.imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
